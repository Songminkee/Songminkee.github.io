---
title: YOLO v2
author: Monch
category: Object Detection_
layout: post
---

 YOLO 9000이라는 제목으로 나온 이 논문은 YOLO의 후속작으로써 YOLO v2라고도 불린다. 괴짜 답게 소제목을 Better, Faster, Stronger로 구성했으며 빠른 속도와 무려 9000 종류의 물체를 구분하는 등 놀라운 성능을 보인다.



<br>

<h2>Better</h2>

YOLO는 분명 빠른 속도를 가지고 있었지만 Fast R-CNN과 같은 region proposal model에 비해 낮은 recall을 갖는다. 저자는 다음과 같은 방법으로 recall과 localization 성능을 올리면서 classification 정확도를 유지했다.

<br>

<h3>1.Batch Normalization</h3>

Batch Normalization은 Gradient vanish/ exploding가 internal covariance shift에 의해 일어난다는 가정("[
How Does Batch Normalization Help Optimization?](https://arxiv.org/abs/1805.11604)"에서는 BN을 써도 internal covariance shift가 일어나기 때문에 직접적인 원인은 아니고 gradient를 좀더 reliable하게 만들어 준다고 한다.)하에 batch 단위로 각 feature들의 평균과 표준편차를 이용해 Normalization하는 방법이다. YOLO 역시 이 방법을 채택하고 dropout을 제거했다. 이로 인해 2%의 mAP를 상승시켰다.

<br>

<h3>2.High Resolution Classifier</h3>

AlexNet에서 시작된 대부분의 분류 모델들은 256 x 256 보다 작은 이미지에서 모델을 훈련했다. YOLO로 역시 pre-trained된 모델을 사용했으며 해당 모델은 224 x 224 크기의 이미지에 대해 훈련되었다. 이로인해 448 x 448 크기의 이미지에 대해 적절한 학습이 되어 있지 않았다. 따라서 YOLO v2는 448 x 448의 크기로 ImageNet에서 약 10 epoch 동안 fine tuning을 진행한다. fine tuning에 의해 약 4%의 mAP를 상승시켰다. 

<br>

<h3>3.Convolutional With Anchor Boxes</h3>

SSD, Faster R-CNN과 달리 YOLO는 가이드라인 없이 bounding box의 정보를 예측 했었다. YOLO v2에서는 다른 모델과 같이 Anchor box를 도입해 가이드라인을 제공한다. 또한, 모델 구조에서 fc layer들을 모두 conv layer로 변경했다. 그리고 마지막 feature map의 경우 큰 object 들은 이미지 중심에 위치하는 경우가 많아 홀수의 크기를 갖는게 좋다고 한다. 이로인해 448 x 448 입력을 416 x 416으로 변경하였다.

anchor box없이 사용했을 때 69.5mAP와 81%의 recall을 얻었고 anchor box를 도입 했을 때 69.2 mAP와 88% recall을 얻었다. 비록 mAP는 낮아졌지만 당초의 목적과 같이 recall의 상승을 얻을 수 있었다.

<br>

<h3>4.Dimension Clusters</h3>

이전의 논문들은 Anchor box의 크기나 종횡비를 사람이 직접 정했다. YOLO v2에서는 단순히 종횡비나 크기를 정해주는 것보다 더 최적의 box가 있을 것이라 판단했고 이를 찾게 된다면 더 쉽게 학습이 이루어 질 것이라고 했다. 논문에서는 training dataset에서 k-mean clustering을 적용해 최적의 anchor box를 찾았다. 일반적인 k-mean clustring은 유클리디안 거리를 사용하지만 bounding box에 대해서는 정확하지 않다. 따라서 거리에 대한 식을 IOU에 대한 식으로 변경했다.


$$
d(box,centroid) = 1-IOU(box,centroid)
$$


<img src="{{'assets/picture/yolo_v2_box1.jpg' | relative_url}}">

<img src="{{'assets/picture/yolo_v2_box2.jpg' | relative_url}}">



k가 5일 때 모델의 복잡성과 높은 recall사이의 적절한 tradeoff가 일어난다고 한다.

<br>

<h3>5. Direct location prediction</h3>

이전의 anchor box들의 loss에는 제한이 없었다. 이로 인해 중심 셀에서 상당히 떨어진 위치에 bounding box를 예측하는 일이 발생하기도 한다. YOLO v2에는 이에대한 제약사항을 추가하기 위해 sigmoid를 적용한다.


$$
b_{x} = \sigma(t_{x}) + c_{x} \\
b_{y} = \sigma(t_{y}) + c_{y} \\
b_{w} = p_{w}e^{t_{w}} \\
b_{h} = p_{h}e^{t_{h}} \\
Pr(object) * IOU(b,object) = \sigma(t_{o})
$$


여기서 $$t_{xx}$$들은 network의 output을 나타낸다. 이로 인해 5%의 성능을 향상 시킬 수 있었다고 한다.

<br>

<h3>6.Find-Grained Features</h3>

<img src="{{'assets/picture/yolo_v2_architecture.jpg' | relative_url}}">

YOLO v2의 최종 output은 13 x 13 grid이다. 이전 버전에 대해 작은 물체에 대한 성능이 뒤떨어진다라는 평이 있었다. 그래서 상위에 26 x 26 크기를 가지는 feature을 4등분 하여 마지막 convolution 전에 합쳐준다(Passthrough layer). 이로인해 1%의 성능을 향상 시킬 수 있었는데 데이터셋의 대부분이 어느정도의 크기를 갖는 점을 생각하면 꽤 많이 오른것 같다.

<br>

<h3>7.Multi-Scale Training</h3>

YOLO v2는 이전 버전과 달리 fully convolutional network이다. 저자는 이 강점을 살려서 다양한 scale에 적응한 network를 만들었다. 저자가 제시하는 방법은 다음과 같다. input image의 size를 항상 고정하지 않고 매 10 batch 단위로 다양한 image size를 갖는다. 이때 이미지 사이즈는 32의 배수로 설정하며 최소 320부터 608의 범위를 갖는다.

아래는 PSACAL VOC 2007 dataset에서 각 이미지 사이즈별 정확도와 속도를 보여준다.

<img src="{{'assets/picture/yolo_v2_table3.jpg.jpg' | relative_url}}">

<br>

<br>

<h3>Reference</h3>

[1] J. Redmon, A. Farhadi. YOLO9000:Better, Faster, Stronger. In CVPR, 2017


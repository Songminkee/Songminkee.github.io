---
title: 2. 머신러닝 프로젝트 처음부터 끝까지
author: Monch
category: handson
layout: post
---

 <h2><b>프로젝트를 어떻게 구성할까?</b></h2>

프로젝트는 다음의 순서로 정의한다.

-  <b>큰 그림을 봐라</b>
-  <b>데이터를 구해라</b>
-  <b>데이터를 분석해라</b>
-  <b>머신러닝 알고리즘을 정의하고 그에 맞는 데이터를 준비해라</b>
-  <b>모델을 선택하고 train해라</b>
-  <b>모델을 상세하게 조정해라</b>
-  <b>솔루션을 제시해라</b>
-  <b>시스템을 론칭하고 모니터링하고 유지보수해라</b>

2장에서는 위의 과정들을 주택가격을 예측하는 예제를 이용해 설명하고 있다.



 <h2><b>큰 그림을 보자</b></h2>

1.큰 그림의 시작은 문제의 정의이다.

조금더 구체화 하면 비지니스의 목적이 정확히 무엇인지다.

책의 예제에서는 이 모델의 출력(예측 가격)이 여러 가지 다른 신호와 함께 다른 머신러닝 시스템에 입력으로 사용되고 뒤따르는 시스템이 해당지역에 투자할 가치가 있는지 결정한다.



2.현재의 솔루션을 이해한다.

현재 상황은 구역 주택 가격을 전문가가 수동으로 추정하고 한팀이 한 구역에 관한 최신 정보를 모으고 있는데 중간 주택 가격을 얻을 수 없을 때는 복잡한 규칙을 사용하여 추정한다고 한다.

이는 비용과 시간이 많이 들고 추정한 결과의 정확도도 낮다.



3.가진 정보들을 조합하여 시스템을 설계한다.

이미 모여진 정보가 있다.(지도? 비지도?) -> 지도학습

값을 예측한다.(분류? 회귀?) -> 회귀

예측에 사용할 특성이 여러개 -> 다중회귀 (multiple regression)

각 구역마다 하나의 값을 예측(단변량, 다변량) -> 단변량 회귀 (univariate regression)



4.성능 측정 지표를 선택한다.

회귀문제의 성능 측정 지표에는 전형적으로 평균 제곱근 오차(Root Mean Square Error, RMSE)와 평균 절대 오차(Mean Absolute Error, MAE)가 있다.

각각의 식은 다음과 같다.

$$RMSE(X,h)= \sqrt{ \frac{1}{m}\sum_{i=1}^m (h(\mathbf{x^{(i)}})-y^{(i)})^2}$$.

$$MAE(X,h)= \frac{1}{m}\sum_{i=1}^m \begin{vmatrix}h(\mathbf{x^{(i)}})-y^{(i)} \end{vmatrix}$$.

둘 모두 예측값과 타깃값의 벡터 사이의 거리를 재는 방법이다. 거리 측정에는 여러 가지 방법 (혹은 Norm)이 가능하다.

- RMSE 계산은 우리에게 친숙한 유클리디안 노름(Euclidean norm), 유클리디안 거리이다. 
  $$l_2$$ norm이라고도 부르며 $$\begin{Vmatrix} \centerdot \end{Vmatrix}_2$$(또는 그냥 $$\begin{Vmatrix} \centerdot \end{Vmatrix}$$)로 표시한다.
- MAE는 $$l_1$$ norm 이라고도 부르며 $$\begin{Vmatrix} \centerdot \end{Vmatrix}_1$$로 표기하고 도시의 구획이 직각으로 나뉘어 있을 때 이 도시의 두 지점 사이의 거리를 측정하는 것과 같아 맨해튼 노름(Manhattan norm)이라고도 한다.
- norm은 일반적으로 $$l_k$$일 때 $$\begin{Vmatrix} \mathbf{v} \end{Vmatrix}_k = ( \begin{vmatrix}v_1 \end{vmatrix}^{k} + \begin{vmatrix} v_2 \end{vmatrix}^{k} + \cdots+ \begin{vmatrix}v_n \end{vmatrix}^{k})^{ \frac{1}{k} }$$이다.
- norm의 지수가 클수록 큰 값의 원소에 치우치며 작은 값은 무시된다. 그래서 RMSE가 이상치에 민감하다. 하지만 반대로 이상치가 적다면 RMSE가 잘 맞는다.



 <h2><b>데이터를 구하자</b></h2>

우선 책과 최대한 동일한 환경을 만들자.

아나콘다를 설치하고 아나콘다 프롬프트 창에서 다음과 같이 입력한다.

개인적으로 문제가 제일 안일어났었던게 3.6.2 버전이라 선호한다. 요즘에는 3.7도 많이 안정되어있을 것이다.

```
conda create --name [환경이름] python==3.6.2

ex) conda create --name hands python==3.6.2
```

다음은 환경을 활성화 하고 필요한 라이브러리를 설치한다.

```
conda activate [환경이름]
pip install pands scipy scikit-learn matplotlib
```

근데 사실 환경을 만들 때 다음과 같이 써도 무방하다.

```
conda create --name [환경이름] python==3.6.2 pands scipy scikit-learn matplotlib
```

다음은 생성된 환경을 주피터 노트북에 연동할 거다.

먼저 ipykernel을 설치하고 등록을한다.

```
pip install ipykernel
python -m ipykernel install --user --name [환경이름] --display-name "[표기하고싶은이름]"
```

이제 주피터로 접속하면 아나콘다로 설치한 환경을 선택할 수 있다.

<img src="{{'assets/picture/jupyer_env.jpg' | relative_url}}">

만약 주피터의 단축키를 알고싶다면 Help-Keyboard Shortcuts를 보자



1.데이터 다운받기

이제 데이터를 다운 받을 것이다. 주소가 책에 나온것과 약간 차이가 있으니 참고하자

```python
import os
import tarfile
import urllib
import pandas as pd

DOWNLOAD_ROOT = "https://github.com/ageron/handson-ml2/raw/master/"
HOUSING_PATH = os.path.join("datasets","housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok=True)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
    


def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path,"housing.csv")
    return pd.read_csv(csv_path)

if not os.path.exists("datasets/housing/housing.csv"):
    print("download")
    fetch_housing_data()
```



2.데이터 구조보기

pandas의 head() method를 이용해 처음 다섯 행을 볼 수 있다.

```python
housing = load_housing_data()
housing.head()
```

<img src="{{'assets/picture/head_method_resul.jpg' | relative_url}}" width="700">

특성은 longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, median_house_value, ocean_proximity 등 10개이다.

pandas의 info() method를 사용하면 데이터에 대한 간략한 설명과 특히 전체 행 수, 각 특성의 데이터 타입과 null이 아닌 값의 개수를 확인하는 데 유용하다.

<img src="{{'assets/picture/info_method_resul.jpg' | relative_url}}">

예를들어 total_bedrooms는 20640의 데이터 중 20433만 이 값을 가지고 있다는 것이다.

ocean_proximity의 경우 Data type이 object로 나와 있는데 csv 파일에서 읽었기 때문에 텍스트의 성격을 가지고 있을 것이다. 이 안에 어떤 카테고리가 있고 각 카테고리마다 얼마나 많은 구역이 있는지 value_counts() method로 확인 할 수 있다.

<img src="{{'assets/picture/value_counts_method_resul.jpg' | relative_url}}">

또한, describe() method를 사용하면 숫자형 특성의 요약 정보를 볼 수 있다.

<img src="{{'assets/picture/describe_method_resul.jpg' | relative_url}}" width ="700">

count는 null 아닌 값의 갯수, mean은 평균, min은 최소값, max는 최대값, std는 표준편차, 숫자%는 백분위수이다.

다음은 각 숫자형 특성을 히스토그램으로 그려볼 것이다.

수평축은 주어진 값의 범위를 보여주고 수직축은 속한 샘플 수를 나타낸다.

매직 명령은 matplotlib이 주피터 자체의 백엔드를 사용하도록 설정하고 이렇게 하면 그래프는 노트북 안에 그려지게 된다.

```python
%matplotlib inline # 주피터 노트북의 매직 명령
import matplotlib.pyplot as plt
housing.hist(bins=50, figsize=(20,15))
plt.show()
```

<img src="{{'assets/picture/histogram_method_result.jpg' | relative_url}}" width="650">

히스토그램을 보면 다음과 같은 사실을 파악할 수 있다.

- Median income (중간 소득) 특성이 US 달러로 표현되어 있지 않다. 스케일을 조정하고 상한이 15(실제는 15.0001), 하한이 0.5 (실제로는 0.4999)가 되도록 만들어져있다. (예를들어 3은 실제로 약 30000달러)
  우리는 앞으로도 이렇게 전처리된 데이터를 다루는 상황을 많이 접할 것이고 데이터가 어떻게 계산된 것인지 이해하고 있어야 한다.
- housing median age(중간 주택 연도)와 median house value(중간 주택 가격) 역시 최대값과 최소값을 한정했다. median house value는 y_label( target )이기 때문에 문제가 될 수 있다. 가격이 한계값을 넘어가지 않도록 모델이 학습될 수 있다는 것이다. 실무에서는 클라이언트 팀(이 시스템의 출력을 사용할 팀)과 함께 검토하는 것이 좋다. 만약 최대값이 넘어가더라도 정확한 예측값이 필요하다고 한다면 선택할 수 있는 방법은 두가지이다.
  - 한계값 밖의 구역에 대한 정확한 label을 구한다.
  - 훈련 세트에서 이런 구역을 제거한다.($500,000가 넘는 값에 대한 예측은 정확도가 매우 낮게 되므로 테스트 세트에서도 제거한다.)
- 특성들의 스케일이 서로 상이하다. 때문에 스케일링이라는 작업을 해야 한다.
- 데이터가 골고루 퍼져있지 않고 히스토그램에 꼬리가 생겼다



3.테스트 세트를 만들자

책의 저자는 테스트 세트를 미리 보는 것을 매우 반대한다. 이 테스트 세트로 일반화 오차를 추정하면 매우 낙관적인 추정이 되며 시스템을 론칭했을 때 기대한 성능이 나오지 않을 것이다. 이를 데이터 스누핑 편향(Data snooping bias)라고 한다.

테스트 세트를 생성하는 일은 이론적으로는 매우 간단하다. 그냥 무작위로 어떤 샘플을 선택해서 데이터 셋의 20% 정도(데이터가 클 수록 이 비율은 줄어든다)를 떼어놓으면 된다.

일반적으로 랜덤 함수를 사용하는데 실행될때마다 테스트 세트가 바뀌는 것을 방지하기 위해 시드(seed)를 사용하기도 한다. 하지만 이 방식은 업데이트된 데이터셋을 사용하려면 문제가 된다.

데이터셋의 업데이트 이후에도 안정적인 훈련/테스트 분할을 위한 일반적인 해결책은 샘플의 식별자를 사용해 테스트 세트로 보낼지 말지 정하는 것이다. 예를들어, 각 샘플마다 식별자의 해시값을 계산하여 해시 최대값의 20%보다 작거나 같은 샘플만 테스트 세트로 보내는 방식이다.

코드는 아래와 같다.

```python
from zlib import crc32
import numpy as np

def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32

def split_train_test_by_id(data,test_ratio,id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: test_set_check(id_,test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]

housing_with_id = housing.reset_index()

train_set,test_set = split_train_test_by_id(housing_with_id,0.2,"index")
```



사이킷런에서는 데이터셋을 여러 서브셋으로 나누는 다양한 방법을 제공한다.

아래의 함수에서는 난수 초기값을 지정(rondom_state 파라미터) 할 수 있고, 행의 개수가 같은 여러 개의 데이터셋을 넘겨서 같은 인덱스를 기반으로 나눌 수 있다(데이터 프레임이 레이블에 따라 여러개로 나뉘어 있을 때 유용하다).

```python
from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
```



이러한 방식들은 데이터셋이 충분히 클때는 일반적으로 괜찮지만, 그렇지 않다면 샘플링 편향이 생길 가능성이 크다. 미국 인구의 51.3%가 여성이고 48.7%가 남성이라면 잘 구성된 설문조사는 샘플에서도 이 비율을 유지해야 한다. 이를 계층적 샘플링(stractified sampling)이라고 한다. 

전체 인구는 계층(strata)라는 동질의 그룹으로 나뉘고, 테스트 세트가 전체 인구를 대표하도록 각 계층에서 올바른 수의 샘플을 추출한다.

전문가가 median_income이 median_house_value를 예측하는 데 매우 중요시 한다고 가정하자.

이 경우 테스트 세트가 전체 데이터셋에 있는 여러 소득 카테고리를 잘 대표해야 한다. 중간 소득이 연속적인 숫자형 특성이므로 소득에 대한 카테고리 특성을 만들어야 한다.

계층별로 데이터셋에 충분한 샘플 수가 있어야한다. 그렇지 않으면 계층의 중요도를 추정하는데 편향이 발생할 것이다. 즉, 너무 많은 계층올 나누면 안 되고 각 계층이 충분히 커야한다. 

책의 예제에서는 카테고리 1은 0~1.5까지로 $15,000 단위로 나누었다.

<img src="{{'assets/picture/stratified_sampling_result.jpg' | relative_url}}">

여기까지 되었으면 사이킷런의 StratifiedShuffleSplit을 사용할 수 있다.

```python
from sklearn.model_selection import StractifiedShuffleSplit

split = StractifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

for train_index, test_index in split.split(housing, housing["income_cat"]):
    strat_train_set = housing.loc[train_index]
    strat_test_set = housing.loc[test_index]
```

이전과 비슷한 비율로 테스트 세트가 뽑혔는지 확인해보자

<img src="{{'assets/picture/stratified_sampling_check_result.jpg' | relative_url}}">

다음의 코드를 입력하면 income_cat 특성을 삭제해서 데이터를 원래 상태로 되돌린다.

```python
for set_ in (strat_train_set,strat_test_set):
    set_.drop("income_cat",axis=1,inplace=True)
```





 <h2><b>데이터를 이해해보자</b></h2>

1.지리적 데이터를 시각화해보자

이번에는 다뤄야할 데이터의 종류를 파악하기 위해 데이터를 살펴본다.

훈련 세트를 손상시키지 않기 위해 복사본을 만들어 사용할 것이다.

```python
housing = strat_train_set.copy()
```

먼저 지리 정보(위도와 경도)를 이용해 모든 구역을 산점도로 만들어 데이터를 시각화 해보자

<img src="{{'assets/picture/longi_lati_plot.jpg' | relative_url}}">

여기서 alpha 값을 설정하면  데이터 포인트가 밀집된 영역을 볼 수 있다.

<img src="{{'assets/picture/longi_lati_plot_alpha.jpg' | relative_url}}">

이제 주택 가격을 나타낼 것이다. 원의 반지름은 구역의 인구를 나타내고(파라미터 s), 색상은 가격을 나타낸다(파라미터 c). 책의 예제에서는 미리 정의된 color map 중 파란색에서 빨간색 까지 범위를 가지는 jet을 사용한다(파라미터 cmap).

<img src="{{'assets/picture/longi_lati_pop_val_result.jpg' | relative_url}}">

그림에서 주택 가격은 지역과 인구 밀도에 관련이 매우 크다는 것을 알 수 있다.

군집(Clustering) 알고리즘을 사용해 주요 군집을 찾고 군집의 중심까지의 거리를 재는 특성을 추가할 수도 있다. 



2.상관관계 조사를 해보자

데이터셋이 너무 크지 않으므로 모든 특성 간의 표준 상관계수(standard correlation coefficient,혹은 Pearson의 r)를 corr() method를 이용해 쉽게 계산할 수 있다.

<img src="{{'assets/picture/corr_method_resul.jpg' | relative_url}}">

상관관계의 범위는 -1~1이고 1에 가까우면 강한 양의 상관관계를 가진다는 뜻이다. 예를들어 median_house_value는 median_income이 올라갈 때 증가하는 경향이 있다. 계수가 -1에 가까우면 강한 음의 상관관계가 있다는 것이고 마지막으로 계수가 0에 가까우면 선형적인 상관관계가 없다는 뜻이다.

하지만 상관관계는 선형적인 관계만 측정한다. 즉, 비선형 관계는 파악할 수 없다.



특성 사이의 상관관계를 파악하는 다른 방법으로는 숫자형 특성 사이에 산점도를 그려주는 pands의 scatter_matrix 함수를 사용하는 것이다.

아래는 median_house_value와 상관관계가 높아 보이는 특성에 대한 예이다.

<img src="{{'assets/picture/scatter_matrix_method_resul.jpg' | relative_url}}">

왼쪽위에서 오른쪽 아래로는 변수 자신에 대한 것이라 그냥 직선이 된다.



이중 가장 상관관계가 강한 median_income만 봐보자

<img src="{{'assets/picture/scatter_income_value.jpg' | relative_url}}">

위 사진에서 두가지를 발견할 수 있다.

- 상관관계가 매우 강하다. 위쪽으로 향하는 경향을 볼 수 있으며 포인트들이 너무 널리 퍼져 있지 않다.
- 가격 제한값이 $500,000에서 수평선으로 잘 보인다. 그리고 부분적으로 $350,000와 $280,000에도 수평선이 보인다. 알고리즘이 데이터에서 이런 이상한 형태를 학습하지 않도록 해당 구역을 제거하는 것이 좋다.



3.특성 조합을 만들어보자

앞서 학습을 하기전 정제해야할 데이터들을 확인했고, 특성 사이에서 유용한 상관관계를 발견했다. 어떤 특성은 꼬리가 두꺼운 분포라서 데이터를 변형해야 한다.

학습전 마지막으로 해볼 수 있는 것은 여러 특성의 조합을 시도해보는 것이다. 예를들어 특정 구역의 방 개수는 가구 수가 얼마나 되는지 모른다면 그다지 유용하지 않다. 정확히 필요한 것은 가구당 방 개수이다. 침대수는 방당 침대수이다. 이를 이용하면 다음과 같은 새로운 특성을 만들어 볼 수 있다.

<img src="{{'assets/picture/new_attribute_resul.jpg' | relative_url}}">

scatter_matrix를 다시 한번 보자

<img src="{{'assets/picture/new_attribute_scatter.jpg' | relative_url}}">

방당 침대의 수와 가격은 음의 상관관계가 조금 생겼다. 하지만 나머지 값은 이전과 크게 다르지 않다.

이 탐색 단계는 아직 완벽하지 않다.
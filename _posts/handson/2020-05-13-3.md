---
title: 3. 분류
author: Monch
category: handson
layout: post
---

 <h2><b>MNIST</b></h2>

딥러닝 책을 읽어봤다면 모를 수가 없는 데이터셋이다.

먼저 MNIST 데이터세트를 다운 받아보자

```python
>>> from sklearn.datasets import fetch_openml
>>> mnist = fetch_openml('mnist_784',version=1)
>>> mnist.keys()
dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'DESCR', 'details', 'categories', 'url'])
```

사이킷런에서 읽어 들인 뎅터셋들은 일반적으로 비슷한 dictionary 구조를 가지고 있다.

- 데이터셋을 설명하는 DESCR 키
- 샘플이 하나의 행, 특성이 하나의 열로 구성된 배열을 가진 data 키
- 레이블 배열을 담은 target 키

이 배열들의 shape을 확인해보자

```python
>>> X, y = mnist["data"],mnist["target"]
>>> X.shape
(70000, 784)
>>> y.shape
(70000,)
```

이미지가 70,000개 있고 각 이미지에는 784개의 특성이 있다.

여기서 784개의 특성은 이미지가 28 X 28이기 때문이다. 각 featrue는 0~255까지의 값을 나타낸다.(gray scale의 픽셀 값)

그러면 이미지로 확인해보자

<img src="{{'assets/picture/mnist_plt.jpg' | relative_url}}">

실제 레이블을 확인해보면 그림과 같이 5이다.

```python
>>> y[0]
'5'
```

현재 문자열로 되어 있으므로 y를 정수형으로 변환해둬야 한다.

```python
>>> import numpy as np
>>> y = y.astype(np.uint8)
>>> y[0]
5
```

위의 예제로 다운받은 MNIST 데이터 셋은 이미 train set(앞에 60000)과 test set(뒤쪽 10000)로 나눠놓아서 따로 만들 필요가 없다.

다음과 같이 미리 지정해놓으면 된다.

```python
X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]
```

훈련세트는 이미 섞여 있어서 모든 교차 검증 폴드를 비슷하게 만든다.

어떤 알고리즘은 훈련 샘플의 순서에 민감해서 섞지 않는게 좋은 경우도 있다.



 <h2><b>이진 분류기 훈련</b></h2>

문제를 단순화해서 5가 맞다 혹은 아니다 두개의 클레스를 구분할 수 있는 이진 분류기(Binary classifier)를 해보자. 분류 작업을 위해 타깃 벡터를 미리 만들어 놓자.

```python
y_train_5 = (y_train==5)
y_test_5 = (y_test==5)
```

이제 분류 모델을 하나 선택해서 훈련시켜보자.

여기서는 사이킷런의 SGDClassifier 클래스를 사용해 확률적 경사 하강법(Stochastic Gradient Descent, SGD) 분류기로 시작했다.

이 분류기는 한 번에 하나씩 훈련 샘플을 독립적으로 처리하기 때문에 매우 큰 데이터셋을 효율적으로 처리하는 장점을 지니고 있다(그래서 온라인 학습에 잘 맞다).

다음과 같이 모델을 만들고 전체 훈련 세트를 사용해 훈련하면 된다.

```python
>>> from sklearn.linear_model import SGDClassifier
>>> sgd_clf = SGDClassifier(random_state = 42)
>>> sgd_clf.fit(X_train, y_train_5)
>>> sgd_clf.predict([some_digit])
array([ True])
```

fit 까지가 훈련이고 predict가 실제 test이다.

random_state는 random 함수의 seed라고 생각하면 편하다.

이제 모델의 성능을 평가해보자



 <h2><b>성능 측정</b></h2>

1.교차 검증을 사용한 정확도 측정

가끔 사이킷런이 제공하는 기능보다 교차 검증 과정을 더 많이 제어해야 할 필요가 있다.

아래의 예제는 사이킷런의 cross_val_score() 함수와 거의 같은 작업을 수행하고 동일한 결과를 출력한다.

```python
>>> from sklearn.model_selection import StratifiedKFold
>>> from sklearn.base import clone
>>> 
>>> skfolds = StratifiedKFold(n_splits=3, random_state=42)
>>> 
>>> for train_index, test_index in skfolds.split(X_train, y_train_5):
>>>     clone_clf = clone(sgd_clf)
>>>     X_train_folds = X_train[train_index]
>>>     y_train_folds = y_train_5[train_index]
>>>     X_test_fold = X_train[test_index]
>>>     y_test_fold = y_train_5[test_index]
>>>     
>>>     clone_clf.fit(X_train_folds,y_train_folds)
>>>     y_pred = clone_clf.predict(X_test_fold)
>>>     n_correct = sum(y_pred == y_test_fold)
>>>     print(n_correct/ len(y_pred))
0.95035
0.96035
0.9604
```

StratifiedKFold는 클래스별 비율이 유지되도록 폴드를 만들기 위해 계층적 샘플링을 수행한다.

매 반복에서 분류기 객체를 복제하여 훈련 폴드로 훈련시키고 테스트 폴드로 예측을 만든다.

그런 다음 올바른 예측의 수를 세어 정확한 예측의 비율을 출력한다.



이번에는 사이킷 런의 cross_val_score() 함수로 폴드가 3개인 k-겹 교차 검증을 사용해 SGDClassifier 모델을 평가해보자.

```python
>>> from sklearn.model_selection import cross_val_score
>>> cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring = "accuracy")
array([0.95035, 0.96035, 0.9604 ])
```

두 결과가 같다는 것을 볼 수 있다.

이번에는 모든 이미지를 '5아님' 클래스로 분류하는 더미 분류기를 만들어본다.

```python
from sklearn.base import BaseEstimator

class Never5Classifier(BaseEstimator):
    def fit(self, X, y=None):
        return self
    def predict(self, X):
        return np.zeros((len(X),1),dtype=bool)
```

모델의 정확도는 다음과 같다.

```python
>>> never_5_clf = Never5Classifier()
>>> cross_val_score(never_5_clf,X_train,y_train_5,cv=3,scoring="accuracy")
array([0.91125, 0.90855, 0.90915])
```

위의 예제에서도 정확도는 90% 이상이다.

하지만 이 예제는 정확도를 분류기의 성능 측정 지표로 선호하지 않는 이유를 보여준다.

이미지의 10% 정도만 숫자 5이기 때문에 무조건 5가 아니다로 예측하면 정확히 맞출 확률이 90%이다.

 특히 불균형한 데이터셋을 다룰 때 더욱 그렇다.



2.오차 행렬

분류기의 성능을 평가하는 더 좋은 방법은 오차 행렬(Confusion matrix)를 조사하는 방법이다.

기본적인 아이디어는 한 클래스의 샘플이 다른 클래스로 분류된 횟수를 세는 것이다.

테스트 세트로 예측을 만들면 안되므로 다음과 같이 cross_val_predict() 함수를 사용한다.

```python
from sklearn.model_selection import cross_val_predict

y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)
```

cross_val_score() 함수처럼 cross_val_predict() 함수는 k-겹 교차 검증을 수행하지만 평가 점수가 아닌 각 테스트 폴드에서 얻은 예측을 반환한다. 즉, 모델이 훈련하는 동안 보지 못했던 데이터에 대해 예측한다.

이제 confusion_matrix() 함수를 사용해 다음과 같이 오차행렬을 만들면 된다.

```python
>>> from sklearn.metrics import confusion_matrix
>>> confusion_matrix(y_train_5,y_train_pred)
array([[53892,   687],
       [ 1891,  3530]], dtype=int64)
```

오차 행렬의 행은 실제 클래스를, 열은 예측한 클래스를 나타낸다.

0번째 행은 5가 아닌 이미지에 대한 것이다(Negative class). 1번째 행은 5인 이미지에 대한 것이다(Positive class). 행,열 별로 다음과 같은 의미와 이름이 있다.

- (0,0) : 53,892개는 5가아니고 5아님으로 정확하게 분류했다.(true negative)
- (0,1) : 687개는 5가 아닌데 5라고 예측되었다. (false positive)
- (1,0) : 1891개는 5인데 5가 아닌 것으로 예측되었다. (false negative)
- (1,1) : 3530개는 5가 맞고 5로 정확하게 예측되었다. (true postive)

정확도가 높을 수록 대각행렬에 가까워 진다.



3.정밀도와 재현율

오차행렬이 많은 정보를 제공해주지만 가끔 더 요약된 지표가 필요할 때도 있다. 

책에서는 몇가지의 지표를 설명한다.

- 양성 예측의 정확도를 분류기의 정밀도(precision)라고 한다. 분류기가 다른 모든 양성 샘플을 무시하는 것에 주의해야 한다.(5로 예측된 것중 5인 비율)

$$
정밀도 = \frac{TP}{TP+FP}
$$

- 분류기가 정확하게 감지한 양성 샘플의 비율로 재현율(recall), 민감도(sensitivity), 진짜 양성 비율(true positive rate, TPR) 등으로 부른다. (5인 것 중 5로 예측된 비율)

$$
재현율 = \frac{TP}{TP+FN}
$$

사이킷런에서는  분류기의 지표를 계싼하는 여러 함수를 제공한다.

```python
>>> from sklearn.metrics import precision_score, recall_score
>>> precision_score(y_train_5,y_train_pred)
0.8370879772350012
>>> recall_score(y_train_5,y_train_pred)
0.6511713705958311
```

정밀도와 재현율을 $$F_{1}$$ score라고 하는 하나의 숫자로 만들 수 있다. 특히 두 분류기를 비교할 때 유용하다.

$$F_{1}$$ 점수는 정밀도와 재현율의 조화 평균(harmonic mean)이다.
$$
F_{1}=\frac{2}{\frac{1}{정밀도}+\frac{1}{재현율}} = 2 \times \frac{정밀도 \times 재현율}{정밀도 + 재현율} = \frac{TP}{TP + \frac{FN+FP}{2}}
$$
f1_score() 함수를 호출하면 확인 할 수 있다.

```python
>>> from sklearn.metrics import f1_score
>>> f1_score(y_train_5,y_train_pred)
0.7325171197343846
```



4.정밀도/재현율 트레이드오프

정밀도와 재현율이 비슷한 분류기에서는 $$F_{1}$$ 점수가 높다. 하지만 이게 항상 바람직하진 않다.

상황에 따라 정밀도가, 혹은 재현율이 더 중요할 수 있다.

예를들어, 아이들에게 안전한 동영상을 걸러내는 분류기를 훈련했을 때, 재현율은 높으나 나쁜 동영상 몇개가 노출되는 것보다 좋은 동영상이 많이 제외되더라도 안전한 것들만 노출시키는 분류기가 선회된다.

또 다른 예로, 감시 카메라를 통해 좀도둑을 잡아내는 분류기를 훈련했을 때, 분류기의 재현율이 99%라면 정밀도가 30%만 되더라도 많은 좀도둑을 잡을 수 있다.

정밀도가 올라가면 재현율은 줄고 반대도 마찬가지이다. 이를 정밀도/재현율 트레이드오프라고 한다.



SGDClassifier가 분류를 어떻게 결정하는지 보며 이 트레이드오프를 이해해보자. 이 분류기는 결정 함수(decision function)를 사용하여 각 샘플의 점수를 계산한다. 이 점수가 임계값보다 크면 샘플을 양성 클래스에 할당하고 그렇지 않으면 음성 클래스에 할당한다.

사이킷런에서 임계값을 직접 정할수는 없지만 예측에 사용한 점수는 확인할 수 있다. 분류기의 predict() method 대신 decision_function() method를 호출하면 각 샘플이 점수를 얻을 수 있다. 이 점수를 기반으로 원하는 임계값을 정해 예측을 만들 수 있다.

```python
>>> y_scores = sgd_clf.decision_function([some_digit])
>>> y_scores
array([2164.22030239])
>>> threshold=0
>>> y_some_digit_pred = (y_scores>threshold)
array([ True])
```

SGDClassifier의 임계값이 0이므로 위 코드는 predict() method와 같은 결과를 반환한다. 임계값을 다음과 같이 늘리면 재현율이 줄어드는 것을 볼 수 있다.

```python
threshold=8000
y_some_digit_pred = (y_scores>threshold)
array([ False])
```

이미지가 실제로 숫자 5이고 임계값이 0일 때는 분류기가 감지 했지만 임계값을 높이면 놓치게 된다.

그렇다면 어떻게 적절한 임계값을 정할 수 있을까? 

이를 위해 먼저 cross_val_predict() 함수를 사용해 훈련 세트에 있는 모든 샘플의 점수를 구해야한다. 하지만 이번에는 예측결과가 아니라 결정 점수를 반환받도록 지정해야 한다.

```python
y_scores = cross_val_predict(sgd_clf,X_train,y_train_5,cv=3,method="decision_function")
```

이 점수로 precision_recall_curve() 함수를 사용해 가능한 모든 임계값에 대해 정밀도와 재현율을 계산할 수 있다.

```
from sklearn.metrics import precision_recall_curve

precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)
```

matplotlib을 사용해 임계값의 함수로 정밀도와 재현율을 그리는 예제는 다음과 같다.

```python
def plot_precision_recall_vs_threshod(precisions,recalls,thresholds):
    plt.plot(thresholds,precisions[:-1],"b--",label="Precision")
    plt.plot(thresholds, recalls[:-1],"g-",label="Recall")
    plt.legend(loc="center right", fontsize=16) 
    plt.xlabel("Threshold", fontsize=16)       
    plt.grid(True)                              
    plt.axis([-50000, 50000, 0, 1])           

recall_90_precision = recalls[np.argmax(precisions >= 0.90)]
threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]

plot_precision_recall_vs_threshod(precisions,recalls,thresholds)
plt.plot([threshold_90_precision, threshold_90_precision], [0., 0.9], "r:")     
plt.plot([-50000, threshold_90_precision], [0.9, 0.9], "r:")                   
plt.plot([-50000, threshold_90_precision], [recall_90_precision, recall_90_precision], "r:")
plt.plot([threshold_90_precision], [0.9], "ro")                          
plt.plot([threshold_90_precision], [recall_90_precision], "ro")
plt.show()
```

<img src="{{'assets/picture/precision_recall_vs_threshold.jpg' | relative_url}}">

좋은 정밀도/재현율을 트레이드오프를 선택하는 다른 방법은 재현율에 대한 정밀도 곡선을 그리는 것이다.

```python
def plot_precision_vs_recall(precisions, recalls):
    plt.plot(recalls, precisions, "b-", linewidth=2)
    plt.xlabel("Recall", fontsize=16)
    plt.ylabel("Precision", fontsize=16)
    plt.axis([0, 1, 0, 1])
    plt.grid(True)

plt.figure(figsize=(8, 6))
plot_precision_vs_recall(precisions, recalls)
plt.plot([0.4368, 0.4368], [0., 0.9], "r:")
plt.plot([0.0, 0.4368], [0.9, 0.9], "r:")
plt.plot([0.4368], [0.9], "ro")
plt.show()
```

<img src="{{'assets/picture/precision_vs_recall.jpg' | relative_url}}">

재현율 80% 근처에서 정밀도가 급격하게 줄어드는데 이 하강점 직전을 정밀도/재현율 트레이드오프로 선택하는 것이 좋다. 예를들어 재현율이 60% 정도인 지점이다. 물론 상황마다 달라진다.



정밀도 90% 달성이 목표라고 가정해보자, 그래프에서 임계값이 약 8000이라는 것을 알 수 있따. 최소한 90% 정밀도가 되는 가장 낮은 임계값을 찾을 수 있다.

```python
threshold_90_precision = thresholds[np.argmax(precisions>=0.90)]
```

 훈련세트에 대한 예측을 만들려면 predict() method를 호출하는 대신 y_scores에서 threshold로 하자.

```python
y_train_pred_90 = (y_scores>=threshold_90_precision)
```

이 예측에 대한 정밀도와 재현율을 확인해보자

```python
>>> precision_score(y_train_5,y_train_pred_90)
0.9000345901072293
>>> recall_score(y_train_5,y_train_pred_90)
0.4799852425751706
```

의도한 대로 90% 이상의 정밀도를 구할 수 있었다.



5.ROC 곡선

수신기 조작 특성(Receiver Operating Characteristic, ROC)는 곡선 이진 분류에서 널리 사용된다. 정밀도/재현율 곡선과 매우 비슷하지만, ROC 곡선은 정밀도에 대한 재현율 곡선이 아니고 거짓 양성 비율(Flase Positive Rate,FPR)에 대한 진짜 양성 비율(True Pisitive Rate,TPR,재현율)의 곡선이다.

정확하게 분류한 음성 샘플의 비율을 진짜 음성 비율(True Negative Rate, TNR)이라하고 1에서 TNR을 뺀값이 FPR과 일치한다.

TNR을 특이도(Specificity)라고도 한다. 따라서 ROC 곡선은 민감도에 대한 1-특이도 그래프이다.

3개의 약어로 표현된것들은 ~인 것중 맞춘/틀린 비율이다.

5라고 한것 중 실제 5인 비율을 나타내는 정밀도는 3개의 약어로 표현되어 있지 않다.

- $$TNR = \frac{TN}{FP+TN}$$ (5가 아닌것중 5가 아니다라고 예측한 비율)
- $$FPR=\frac{FP}{FP+TN}=\frac{FP+TN-TN}{FP+TN}=1-\frac{TN}{FP+TN}=1-TNR$$ (5가 아닌것중 5라고 예측한 비율) 

ROC 곡선을 그리려면 먼저 roc_curve() 함수를 사용해 여러 임계값에서 TPR과 FPR을 계산해야 한다.

```python
from sklearn.metrics import roc_curve

fpr,tpr,thresholds = roc_curve(y_train_5,y_scores)
```

이제 matplotlib을 사용해 TPR에 대한 FPR 곡선을 그려보자.

```python
def plot_roc_curve(fpr, tpr, label=None):
    plt.plot(fpr, tpr, linewidth=2, label=label)
    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal
    plt.axis([0, 1, 0, 1])                                    
    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) 
    plt.ylabel('True Positive Rate (Recall)', fontsize=16)
    plt.grid(True)                                           

plt.figure(figsize=(8, 6))                        
plot_roc_curve(fpr, tpr)
plt.plot([4.837e-3, 4.837e-3], [0., 0.4368], "r:") 
plt.plot([0.0, 4.837e-3], [0.4368, 0.4368], "r:")  
plt.plot([4.837e-3], [0.4368], "ro")               
plt.show()
```

<img src="{{'assets/picture/fpr_vs_tpr.jpg' | relative_url}}">

여기에서도 트레이드오프가 있다. 재현율(TPR)이 높을수록 분류기가 만드는 거짓 양성(FPR)이 늘어난다. 점선은 완전한 랜덤 분류기의 ROC 곡선을 뜻한다.

좋은 분류기는 이 점선에서 최대한 멀리 떨어져 있어야한다. (5가 아닌데 5라고 한 비율은 낮아야 하고 5인 것중 5라고 한 비율은 높아야 한다.)

곡선 아래의 면적(Area Under the Curve, AUC)을 측정하면 분류기들을 비교할 수 있다. 완벽한 분류기는 ROC의 AUC가 1이고 완전한 랜덤 분류기는 0.5이다. 사이킷런은 ROC의 AUC를 계산하는 함수를 제공한다.

```python
>>> from sklearn.metrics import roc_auc_score
>>> roc_auc_score(y_train_5,y_scores)
0.9604938554008616
```

일반적으로 Positive class가 드물거나 False Negative보다 False Positive가 더 중요할 때 PR(정밀도/재현율) 곡선을 사용하고 그렇지 않으면 ROC 곡선을 사용한다. 위의 예제에서는 Negative에 비해 Positive class가 드문 경우이기 때문에 PR 곡선을 사용하는 것이 더 좋다.



RandomForestClassifier를 훈련시켜 SGDClassifier의 ROC 곡선과 ROC AUC 점수를 비교해보자.

작동방식의 차이 때문에 RandomForestClassifier에는 decision_function() method가 없기 때문에 predict_proba() method를 사용해야 한다.

predict_proba() method는 sample이 행, class가 열이고 sample이 주어진 class에 속할 확률을 담은 배열을 반환한다.(ex. 어떤 이미지가 5일 확률 70%)

```python
>>> from sklearn.ensemble import RandomForestClassifier
>>> 
>>> forest_clf = RandomForestClassifier(random_state=42)
>>> y_probas_forest = cross_val_predict(forest_clf, X_train,
    y_train_5,cv=3,method="predict_proba")
[[0.11 0.89]
 [0.99 0.01]
 [0.96 0.04]
 ...
 [0.02 0.98]
 [0.92 0.08]
 [0.94 0.06]]
```

roc_curve() 함수는 레이블과 점수를 기대한다. 하지만 점수 대신 클래스 확률을 전달할 수 있다. 여기서는 Positive class 확률을 점수로 사용한다.

```python
y_scores_forest=y_probas_forest[:,1]
fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)
```

matplotlib을 사용해 그리면 다음과 같다.

```
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, "b:", linewidth=2, label="SGD")
plot_roc_curve(fpr_forest, tpr_forest, "Random Forest")
plt.plot([4.837e-3, 4.837e-3], [0., 0.4368], "r:")
plt.plot([0.0, 4.837e-3], [0.4368, 0.4368], "r:")
plt.plot([4.837e-3], [0.4368], "ro")
plt.plot([4.837e-3, 4.837e-3], [0., 0.9487], "r:")
plt.plot([4.837e-3], [0.9487], "ro")
plt.grid(True)
plt.legend(loc="lower right", fontsize=16)
plt.show()
```

<img src="{{'assets/picture/fpr_vs_tpr_sgd_rf.jpg' | relative_url}}">

그림으로 판단하면 RandomForestClassifier의 ROC 곡선이 왼쪽 위 모서리에 더 가까워 SGDClassifier보다 훨씬 좋아 보인다. 실제로 ROC AUC 점수를 보면 다음과 같다.

```python
>>> roc_auc_score(y_train_5,y_scores_forest)
0.9983436731328145
```


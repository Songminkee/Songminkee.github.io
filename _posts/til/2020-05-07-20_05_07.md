---
title: 20/05/07
author: Monch
category: TIL
layout: post
---







종만북과 밑바닥2를 공부했다. 

종만북 19장의 ITES문제는 long long 자료형 썼더니 예제대로 안나와서 당황했다.

unsigned int를 썼어야 했다니....

밑바닥2 8장의 attension 부분이 길어서 2일에 나눠 공부하려고 했는데 뭔가 인상적이라서 쭉 읽어 버렸다.

'attension is all you need' 논문과 NTM은 tensorflow kr에서 이름만 봤었는데 엄청 인상적이었다.

솔직히 책에서 다 다루지 않고 개념적인 설명만 해줘서 '아 이런 느낌이구나'하는 정도 뿐이다.

나중에 시간나면 두 논문도 읽어봐야 겠다.

이제 밑바닥2는 끝내고 핸즈온 머신러닝책이 도착해서 읽어 봐야 겠다.

오늘은 여인수를 정리 하려고 했는데 밑바닥2 이해하며 읽는데에 시간을 다 보내버렸다.

내일은 좀더 많은 시간 투자해서 공부해야겠다.



#### TID

---

- 종만북 19장

- 밑바닥 딥러닝2 - 8장

  - 얼라인먼트 (Alignment) 
    ‘나=I’, ‘고양이=cat’ 과같이 단어나 문구의 대응 관계를 나타내는 정보. 원래는 수작업으로 만들었지만 어텐션에서는 기계에 의한 자동화라는 흐름이 되었다.

  - Attension 기법
    원하는 정보에 주목하는 기법. 글로 쓰면 이해 될지 모르겠다. peeky에서 lstm과 affine 계층 사이에 attension 계층을 추가한다. lstm에는 input으로 encoder의 마지막 은닉층이 들어간다.

    attension에는 lstm의 output과 encoder의 모든 은닉층이 들어간다.
    attension은 또 2개로 다시 나뉜다.
    1.Attension weight 층
    lstm의 output과 encoder의 모든 은닉층을 하다마르곱으로 각 은닉층과 내적을 한다. 벡터의 유사도를 보는 것이다. 이러한 방식으로 인해 선택임에도 미분가능한 방식이 되는 것이다. 이후 값을 정규화 하기 위해 softmax를 사용한다.

    2.weight sum에서는 다시 attension weight 층의 output과 내적을 한다. 
    이 Attension 계층의 output과 lstm의 output이 Affine 계층으로 들어간다.
    물론 위치가 항상 이런식은 아니다 본인이 정보를 주고 싶은 곳과 연결하면 된다. 결과는 해보지 않으면 모른다.

  - 양방향 LSTM
    기존의 lstm 외에 역방향의 lstm계층을 추가한다. 솔직히 reverse랑 뭐가 다른지 모르겠다.

  - Skip connection, residual connection, shor-cut (스킵 연결, 잔차 연결, 숏컷) 다같은말
    계층을 넘어 연결하는 방법 Unet의 skip connection과 똑같다.
    덧셈은 역전파 시 기울기를 그대로 흘려보내므로 모델의 깊이가 깊어져도 기울기가 소실되지 않는다.

  - Transformer 모델 (Self-Attention) 
    기존의 이전의 시각에 계산한 결과를 순서대로 계산하는 RNN 기반의 방식들은 병렬로 계산하는게 불가능하다. 그래서 RNN을 없애는 연구의 일환으로 “Attention is all you need”라는 논문에서 제안한 모델이다. 논문 제목이 말해주듯 RNN이 아닌 Attensiondmf 이용하는데 ‘자신에 대한 주목’이 된다. 즉, 하나의 시계열 데이터를 대상으로 한 어텐션으로 하나의 시계열 데이터 내에서 각 원소가 다른 원소들과 어떻게 관련되는지를 살펴보자는 취지다. 추가로 이 논문에서는 Attension외에도 Skip connection, Layer Normalization, Positional Encoding과 같은 기법이 사용되었다.
    구글의 GNMT보다 학습 시간을 큰 폭으로 줄였고 번역 품질도 상당히 끌어올렸다.

  - Neural Turing Machine(NTM)
    RNN이나 LSTM은 내부 상태를 활용해 시계열 데이터를 기억할 수 있다. 그러나 내부 상태는 길이가 고정이라서 채워 넣을 수 있는 정보량이 제한적이다. 그래서 RNN 외부에 기억 장치(메모리)를 두고 필요한 정보를 사용하는 방식이다.
    Attension을 이용하면 Encoder와 Decoder는 ‘메모리 조작’과 같은 작업을 수행한다. 즉, Encoder가 필요한 정보를 메모리에 쓰고, Decoder는 그 메모리로부터 필요한 정보를 읽어 들인다고 해석할 수 있다.
    RNN의 외부에 정보 저장용 메모리 기능을 배치하고, Attension을 이용해 그 메모리로부터 필요한 정보를 읽거나 쓰는 방법이다. 
    컨트롤러라는 기능이 나온다. 여기서 컨트롤러는 컴퓨터(혹은 튜링머신)와 같은 능력을 얻는데 이 능력은 다음 두가지 이다.

    - 필요한 정보를 쓰거나 불필요한 정보를 지우는 능력
    - 필요한 정보를 다시 읽어 들이는 능력

    이렇게 하면 각 노드가 필요한 위치의 데이터를 읽고 쓸 수 있게 된다. 즉 , 원하는 목적지로 이동할 수 있다. 이러한 메모리 조작을 ‘미분 가능’한 계산으로 구축했다.
    Write Head와 Read Head라는 구조가 나온다. 여기서 Attension을 사용한다. LSTM 계층이 ‘컨트롤러’가 되어 NTM의 주된 처리를 수행한다. 그리고 각 시각에서 LSTM 계층의 은닉 상태를 Write Head 계층이 받아서 필요한 정보를 메모리에 쓴다. 그런 다음 Read Head 계층이 메모리로부터 중요한 정보를 읽어 들여 다음 시각의 LSTM 계층으로 전달한다. 



#### TODO list

---

- 사진 latex 문법으로 교체
- 종만북-Algospot 1문제 이상 풀기
- 핸즈온 머신러닝
- 여인수 정리

  

